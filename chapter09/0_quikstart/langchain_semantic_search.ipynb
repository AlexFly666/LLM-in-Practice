{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "代码示例说明:\n",
    "\n",
    "这个代码示例演示了如何使用 LangChain 构建一个基于 PDF 文档的语义搜索引擎。它包含了文档加载、分割、嵌入生成、向量存储和检索等关键步骤。\n",
    "\n",
    "文档加载: 使用 PyPDFLoader 从 PDF 文件加载文档，每页一个文档。\n",
    "文档分割: 使用 RecursiveCharacterTextSplitter 将文档分割成更小的块，以便更好地进行语义搜索。\n",
    "嵌入生成: 使用 OpenAIEmbeddings 将文本块转换为嵌入向量，这些向量捕捉了文本的语义信息。\n",
    "向量存储: 使用 Chroma 存储嵌入向量，以便进行高效的相似性搜索。\n",
    "向量检索: 使用向量存储进行相似性搜索，找到与查询最相关的文档块。\n",
    "检索器: 创建可重用的检索器对象，方便进行批量查询。\n",
    "解决的问题:\n",
    "\n",
    "传统的关键词搜索只能找到包含特定关键词的文档，而语义搜索则可以找到含义与查询相关的文档，即使文档中没有包含查询的关键词。这个代码示例展示了如何使用 LangChain 构建一个语义搜索引擎，从而更准确地找到用户感兴趣的信息。\n",
    "\n",
    "达到的效果:\n",
    "\n",
    "通过这个代码示例，你可以：\n",
    "\n",
    "加载和处理 PDF 文档。\n",
    "将文本转换为嵌入向量，捕捉语义信息。\n",
    "使用向量存储进行高效的相似性搜索。\n",
    "构建一个可重用的检索器对象。\n",
    "这个示例可以帮助你理解 LangChain 的核心概念，并为构建更复杂的自然语言处理应用打下基础。  代码中添加了详细的中文注释，解释了每个步骤的作用和目的，方便理解和学习。\n",
    "\n"
   ],
   "id": "b1e322cfddce5c55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install langchain \\\n",
    "            langchain-community \\\n",
    "            langchain-openai \\\n",
    "            langchain-text-splitters \\\n",
    "            pypdf \\\n",
    "            chromadb"
   ],
   "id": "7d0b7e900eef2605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 导入必要的库\n",
    "import getpass\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 设置 LangSmith 追踪，用于调试和监控 LangChain 应用\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# 1. 加载文档\n",
    "# 使用 PyPDFLoader 从 PDF 文件加载文档\n",
    "file_path = \"../example_data/nke-10k-2023.pdf\"  # PDF 文件路径\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"加载了 {len(docs)} 个文档（每页一个文档）\")\n",
    "\n",
    "# 打印第一个文档的内容和元数据\n",
    "print(f\"第一个文档的内容（前200个字符）：\\n{docs[0].page_content[:200]}\\n\")\n",
    "print(f\"第一个文档的元数据：\\n{docs[0].metadata}\")\n",
    "\n",
    "\n",
    "# 2. 分割文档\n",
    "# 使用 RecursiveCharacterTextSplitter 将文档分割成更小的块\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"将文档分割成 {len(all_splits)} 个块\")\n",
    "\n",
    "# 3. 生成嵌入\n",
    "# 使用 OpenAIEmbeddings 创建嵌入模型\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 示例：生成两个文本块的嵌入向量并比较长度\n",
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)  # 确保向量长度一致\n",
    "print(f\"生成的向量长度为 {len(vector_1)}\\n\")\n",
    "print(f\"第一个向量的前10个元素：\\n{vector_1[:10]}\")\n",
    "\n",
    "# 4. 创建向量存储\n",
    "# 使用 Chroma 创建向量存储\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# 将分割后的文档块添加到向量存储\n",
    "ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(f\"将 {len(all_splits)} 个文档块添加到向量存储\")\n",
    "\n",
    "\n",
    "# 5. 使用向量存储进行搜索\n",
    "# 示例：使用相似性搜索查询文档\n",
    "results = vector_store.similarity_search(\n",
    "    \"How many distribution centers does Nike have in the US?\"\n",
    ")\n",
    "\n",
    "print(f\"相似性搜索结果：\\n{results[0]}\")\n",
    "\n",
    "# 示例：异步查询\n",
    "async def async_search():\n",
    "    results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
    "    print(f\"异步搜索结果：\\n{results[0]}\")\n",
    "\n",
    "#import asyncio\n",
    "#asyncio.run(async_search())\n",
    "\n",
    "\n",
    "# 示例：返回相似性得分\n",
    "results = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")\n",
    "doc, score = results[0]\n",
    "print(f\"相似性得分：{score}\\n\")\n",
    "print(f\"带得分的搜索结果：\\n{doc}\")\n",
    "\n",
    "# 示例：使用嵌入向量搜索\n",
    "embedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n",
    "results = vector_store.similarity_search_by_vector(embedding)\n",
    "print(f\"基于向量的搜索结果：\\n{results[0]}\")\n",
    "\n",
    "\n",
    "# 6. 创建检索器\n",
    "# 自定义检索器示例\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    return vector_store.similarity_search(query, k=1)\n",
    "\n",
    "# 批量检索示例\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"How many distribution centers does Nike have in the US?\",\n",
    "        \"When was Nike incorporated?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 使用 as_retriever 创建检索器\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "\n",
    "# 批量检索示例\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"How many distribution centers does Nike have in the US?\",\n",
    "        \"When was Nike incorporated?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"检索器示例完成\")"
   ],
   "id": "33096802b2b2a3e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
