{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexFly666/LLM-in-Practice/blob/main/chapter09/0_quikstart/rag_react_agent_chroma_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "bff4134caab8eb57",
        "outputId": "b7a96a00-b529-4865-d8ef-9eb2c4f58a76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.18 in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: langchain-community==0.3.17 in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
            "Requirement already satisfied: langchain-core==0.3.34 in /usr/local/lib/python3.11/dist-packages (0.3.34)\n",
            "Requirement already satisfied: langchain-openai==0.3.4 in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (3.11.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.18) (1.26.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.17) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.17) (2.7.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.17) (0.4.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.34) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.34) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.34) (4.12.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.4) (1.61.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.4) (0.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.18) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.18) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.18) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.18) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.18) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.18) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.17) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.17) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.34) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.18) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.18) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.18) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.18) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.4) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.4) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.4) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.4) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.4) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.18) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.18) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.17) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.18) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.18) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.18) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.18) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.18) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.4) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.18) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.18) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.17) (1.0.0)\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# LangChain 最新版本\n",
        "# !pip install langchain \\\n",
        "#     langchain-community \\\n",
        "#     langchain-core \\\n",
        "#     langchain-openai\n",
        "# LangChain V0.3版本\n",
        "!pip install langchain==0.3.18 \\\n",
        "    langchain-community==0.3.17 \\\n",
        "    langchain-core==0.3.34 \\\n",
        "    langchain-openai==0.3.4"
      ],
      "id": "bff4134caab8eb57"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-core \\\n",
        "    langchain-openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wChPMdAKARN7",
        "outputId": "1c5d7dd3-f007-434b-9544-48cc1f5c6e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wChPMdAKARN7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.18\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-community\n",
            "---\n",
            "Name: langchain-community\n",
            "Version: 0.3.17\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, httpx-sse, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-core\n",
            "Version: 0.3.34\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity, typing-extensions\n",
            "Required-by: langchain, langchain-community, langchain-openai, langchain-text-splitters\n",
            "---\n",
            "Name: langchain-openai\n",
            "Version: 0.3.4\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入操作系统库，用于文件路径操作等\n",
        "import os\n",
        "\n",
        "# 导入Langchain库中的HuggingFaceEmbeddings类，用于使用HuggingFace模型生成文本嵌入\n",
        "from langchain.embeddings import HuggingFaceEmbeddings,HuggingFaceBgeEmbeddings\n",
        "# 导入Langchain库中的CharacterTextSplitter类，用于按字符分割文本\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# 导入Langchain Community库中的TextLoader类，用于加载文本文件\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "# 导入LangchainCommunity库中的Chroma类，用于创建和管理Chroma向量数据库\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# 导入Langchain OpenAI库中的OpenAIEmbeddings类，用于使用OpenAI模型生成文本嵌入\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# 获取当前脚本文件所在的目录\n",
        "current_dir = os.getcwd()\n",
        "# 构建书籍文件（odyssey.txt）的完整路径，假设书籍文件位于当前目录下的 \"books\" 文件夹中\n",
        "file_path = os.path.join(current_dir, \"data\", \"books\", \"xiyouji.txt\")\n",
        "# 构建持久化向量数据库的目录路径，数据库文件将存储在当前目录下的 \"db\" 文件夹中\n",
        "db_dir = os.path.join(current_dir, \"db\")\n",
        "\n",
        "# 检查书籍文件是否存在\n",
        "if not os.path.exists(file_path):\n",
        "    # 如果书籍文件不存在，则抛出 FileNotFoundError 异常，提示用户检查文件路径\n",
        "    raise FileNotFoundError(\n",
        "        f\"The file {file_path} does not exist. Please check the path.\"\n",
        "    )\n",
        "\n",
        "# 使用 TextLoader 加载文本文件内容\n",
        "loader = TextLoader(file_path)\n",
        "# 将加载的文本内容存储在documents变量中，TextLoader将文件内容加载为Document对象列表\n",
        "documents = loader.load()\n",
        "\n",
        "# 创建CharacterTextSplitter实例，用于将文档分割成更小的文本块\n",
        "# chunk_size=1000 表示每个文本块的大小为1000个字符\n",
        "# chunk_overlap=0 表示文本块之间没有重叠部分\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "# 使用text_splitter将加载的文档分割成文本块，存储在docs变量中，docs是Document对象列表\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# 打印文档分割块的信息\n",
        "print(\"\\n--- Document Chunks Information ---\")\n",
        "# 打印分割后的文档块数量\n",
        "print(f\"Number of document chunks: {len(docs)}\")\n",
        "# 打印第一个文档块的内容示例，用于预览分割效果\n",
        "# print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
        "\n",
        "\n",
        "# 定义函数create_vector_store，用于创建并持久化向量数据库\n",
        "# 参数 docs：文档块列表，embeddings：嵌入模型，store_name：向量数据库存储名称\n",
        "def create_vector_store(docs, embeddings, store_name):\n",
        "    # 构建持久化向量数据库的完整目录路径\n",
        "    persistent_directory = os.path.join(db_dir, store_name)\n",
        "    # 检查持久化目录是否已存在\n",
        "    if not os.path.exists(persistent_directory):\n",
        "        # 如果持久化目录不存在，则表示需要创建新的向量数据库\n",
        "        print(f\"\\n--- Creating vector store {store_name} ---\")\n",
        "        # 使用 Chroma.from_documents 方法从文档块创建向量数据库\n",
        "        # docs：文档块列表，embeddings：嵌入模型，persist_directory：持久化目录\n",
        "        Chroma.from_documents(\n",
        "            docs, embeddings, persist_directory=persistent_directory)\n",
        "        # 打印向量数据库创建完成的提示信息\n",
        "        print(f\"--- Finished creating vector store {store_name} ---\")\n",
        "    else:\n",
        "        # 如果持久化目录已存在，则表示向量数据库已存在，无需重新初始化\n",
        "        print(\n",
        "            f\"Vector store {store_name} already exists. No need to initialize.\")\n",
        "\n",
        "\n",
        "# 1. OpenAI Embeddings\n",
        "# 使用 OpenAI 的嵌入模型。\n",
        "# 适用于通用目的的嵌入，具有高准确性。\n",
        "# 注意：使用 OpenAI 嵌入的成本取决于您的 OpenAI API 使用量和定价计划。\n",
        "# 定价：https://openai.com/api/pricing/\n",
        "# print(\"\\n--- Using OpenAI Embeddings ---\")\n",
        "# # 创建 OpenAIEmbeddings 实例，使用 \"text-embedding-ada-002\" 模型\n",
        "# # openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "# openai_embeddings = OpenAIEmbeddings(\n",
        "#     api_key=\"sk-paSHgQoVeKag1rou9d81Fa2f534940C1Ba394f02C45aF3D2\",\n",
        "#     base_url=\"https://vip.apiyi.com/v1\",\n",
        "#     model=\"text-embedding-3-small\"\n",
        "# )\n",
        "# # 调用create_vector_store函数，使用OpenAI嵌入模型创建名为 \"chroma_db_openai\" 的向量数据库\n",
        "# create_vector_store(docs, openai_embeddings, \"chroma_db_openai\")\n",
        "\n",
        "# 2. Hugging Face Transformers\n",
        "# 使用 Hugging Face 库的模型。\n",
        "# 非常适合利用各种模型来完成不同的任务。\n",
        "# 注意：在您的机器本地运行 Hugging Face模型除了使用您的计算资源外，没有其他直接成本。\n",
        "# 注意：在 https://huggingface.co/models?other=embeddings 查找其他模型\n",
        "print(\"\\n--- Using Hugging Face Transformers ---\")\n",
        "# 创建 HuggingFaceEmbeddings 实例，使用 \"sentence-transformers/all-mpnet-base-v2\" 模型\n",
        "# huggingface_embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "# )\n",
        "model_name = \"BAAI/bge-large-zh-v1.5\"\n",
        "# 根据你的需要去选择设备\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs,\n",
        "    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n",
        ")\n",
        "# 调用create_vector_store函数，使用Hugging Face嵌入模型创建名为 \"chroma_db_huggingface\" 的向量数据库\n",
        "create_vector_store(docs, huggingface_embeddings, \"chroma_db_huggingface\")\n",
        "\n",
        "# 打印嵌入演示完成的提示信息\n",
        "print(\"Embedding demonstrations for OpenAI and Hugging Face completed.\")\n",
        "\n",
        "\n",
        "# 定义函数query_vector_store，用于查询向量数据库\n",
        "# 参数 store_name：向量数据库存储名称，query：查询语句，embedding_function：嵌入函数\n",
        "def query_vector_store(store_name, query, embedding_function):\n",
        "    # 构建持久化向量数据库的完整目录路径\n",
        "    persistent_directory = os.path.join(db_dir, store_name)\n",
        "    # 检查持久化目录是否已存在，以确保向量数据库存在\n",
        "    if os.path.exists(persistent_directory):\n",
        "        # 如果持久化目录存在，则表示向量数据库存在，可以进行查询\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        # 加载已持久化的向量数据库\n",
        "        db = Chroma(\n",
        "            persist_directory=persistent_directory,\n",
        "            embedding_function=embedding_function,\n",
        "        )\n",
        "        # 创建检索器，用于从向量数据库中检索相关文档\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=\"similarity_score_threshold\",\n",
        "            # search_kwargs 参数用于设置检索参数\n",
        "            # k=3 表示检索 top 3 个最相关的文档\n",
        "            # score_threshold=0.1 表示相似度得分阈值为 0.1，只有相似度得分高于 0.1 的文档才会被检索出来\n",
        "            search_kwargs={\"k\": 3, \"score_threshold\": 0.001},\n",
        "        )\n",
        "        # 使用检索器根据查询语句检索相关文档\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # 打印相关文档的标题\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        # 遍历检索到的相关文档，并打印文档内容和元数据\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            # 打印文档内容\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            # 检查文档是否包含元数据\n",
        "            if doc.metadata:\n",
        "                # 打印文档来源，如果元数据中包含 'source' 字段，则打印来源信息，否则打印 'Unknown'\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "    else:\n",
        "        # 如果持久化目录不存在，则表示向量数据库不存在，打印提示信息\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "\n",
        "# 定义用户的查询问题\n",
        "query = \"孙悟空的师傅是谁?\"\n",
        "\n",
        "# 使用 OpenAI 嵌入查询 \"chroma_db_openai\" 向量数据库\n",
        "# query_vector_store(\"chroma_db_openai\", query, openai_embeddings)\n",
        "# 使用 Hugging Face 嵌入查询 \"chroma_db_huggingface\" 向量数据库\n",
        "query_vector_store(\"chroma_db_huggingface\", query, huggingface_embeddings)\n",
        "\n",
        "# 打印查询演示完成的提示信息\n",
        "print(\"Querying demonstrations completed.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j0PzbiHVGsoz",
        "outputId": "92c11fb8-096e-44c1-d1e9-1bd726eea26e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "id": "j0PzbiHVGsoz",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Document Chunks Information ---\n",
            "Number of document chunks: 1\n",
            "\n",
            "--- Using Hugging Face Transformers ---\n",
            "\n",
            "--- Creating vector store chroma_db_huggingface ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidDimensionException",
          "evalue": "Embedding dimension 1024 does not match collection dimensionality 768",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidDimensionException\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-5f35ad7251f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# 调用create_vector_store函数，使用Hugging Face嵌入模型创建名为 \"chroma_db_huggingface\" 的向量数据库\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mcreate_vector_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuggingface_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chroma_db_huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# 打印嵌入演示完成的提示信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-5f35ad7251f5>\u001b[0m in \u001b[0;36mcreate_vector_store\u001b[0;34m(docs, embeddings, store_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# 使用 Chroma.from_documents 方法从文档块创建向量数据库\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# docs：文档块列表，embeddings：嵌入模型，persist_directory：持久化目录\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         Chroma.from_documents(\n\u001b[0m\u001b[1;32m     61\u001b[0m             docs, embeddings, persist_directory=persistent_directory)\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# 打印向量数据库创建完成的提示信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    888\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             ):\n\u001b[0;32m--> 843\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m    844\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mids_with_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_empty_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     self._collection.upsert(\n\u001b[0m\u001b[1;32m    300\u001b[0m                         \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_with_metadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    342\u001b[0m         )\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         self._client._upsert(\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0mcollection_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupsert_request\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rate_limit_enforcer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/rate_limit/simple_rate_limit/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    550\u001b[0m             )\n\u001b[1;32m    551\u001b[0m         )\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_embedding_record_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords_to_submit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         self._quota_enforcer.enforce(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_validate_embedding_record_set\u001b[0;34m(self, collection, records)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 self._validate_dimension(\n\u001b[0m\u001b[1;32m    880\u001b[0m                     \u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_validate_dimension\u001b[0;34m(self, collection, dim, update)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dimension\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dimension\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m             raise InvalidDimensionException(\n\u001b[0m\u001b[1;32m    897\u001b[0m                 \u001b[0;34mf\"Embedding dimension {dim} does not match collection dimensionality {collection['dimension']}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             )\n",
            "\u001b[0;31mInvalidDimensionException\u001b[0m: Embedding dimension 1024 does not match collection dimensionality 768"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码实现了一个**具备对话历史记录功能的检索增强生成 (RAG) 聊天机器人**。 简单来说，它是一个可以和你**像真人一样聊天**的 AI 助手，并且它还很聪明，因为：\n",
        "\n",
        "1.  **它能记住你们之前的对话:**  不像普通的聊天机器人，它不会忘记你之前说过什么。如果你问了一个问题，然后在之后的问题里用 \"它\" 或 \"他\" ，它也能明白你在指代什么，因为它记住了对话的上下文。 **[解决问题：理解对话上下文，使多轮对话更自然]**  **[达到效果：更流畅、更连贯的对话体验]**\n",
        "\n",
        "2.  **它懂得很多东西，而且知识可以更新:**  它不仅仅依靠它自己训练时学到的知识，更重要的是，它能连接到一个**知识库 (就像一个图书馆)**，从中查找信息来回答你的问题。 如果知识库更新了，它的知识也会跟着更新，不会过时。 **[解决问题：语言模型知识有限，无法回答特定领域或最新信息问题]**  **[达到效果：能够回答更广泛、更专业、更及时的提问]**\n",
        "\n",
        "3.  **它用检索到的信息来生成答案，所以答案更可靠:**  当它回答问题时，不是随便乱说，而是会先从知识库里找到相关的资料，然后**基于这些资料来组织答案**。 这就像写论文时引用参考文献一样，答案更有依据，也更可信。  **[解决问题：生成答案缺乏依据，可能不准确或不可靠]**  **[达到效果：提高答案的准确性和可信度]**\n",
        "\n",
        "**总结来说，这段代码就像是打造了一个聪明的 AI 助手，它通过记住对话、查找资料、并基于资料生成答案，从而能够更好地理解你的问题并给出有用的回答。**\n",
        "\n",
        "**代码主要做了以下事情：**\n",
        "\n",
        "*   **加载知识库:**  代码连接到一个已经准备好的知识库 (向量数据库)，这个知识库里存储了很多信息，等待被检索和利用。\n",
        "*   **创建智能检索工具:**  代码创建了一个 \"历史感知检索器\"，这个工具可以理解你问题的含义，包括问题中可能包含的上下文信息，并在知识库中找到最相关的资料。\n",
        "*   **创建智能问答工具:**  代码创建了一个 \"问答链\"，这个工具会利用找到的资料，结合强大的语言模型 (例如 gpt-4o)，来生成简洁明了的答案。\n",
        "*   **连接检索和问答工具:**  代码将检索工具和问答工具连接起来，形成一个完整的 **RAG 聊天机器人系统**。\n",
        "*   **实现持续对话功能:**  代码提供了一个简单的聊天界面，你可以通过输入文字与 AI 助手进行多轮对话。\n",
        "\n",
        "**最终达到的效果：**\n",
        "\n",
        "*   **用户可以与 AI 进行自然的、有上下文的对话。**\n",
        "*   **AI 能够回答各种基于知识库的问题，即使是复杂或需要专业知识的问题。**\n",
        "*   **AI 的答案简洁、明了、并且有信息来源支撑，更可靠。**\n",
        "\n",
        "**解决的主要问题：**\n",
        "\n",
        "*   **传统聊天机器人无法理解对话上下文的问题。**\n",
        "*   **语言模型自身知识有限，无法回答特定领域或最新信息的问题。**\n",
        "*   **生成答案缺乏依据，可能不准确或不可靠。**\n",
        "*   **构建复杂的 RAG 聊天机器人流程比较繁琐。** （代码通过使用 Langchain 框架简化了构建过程）\n",
        "\n",
        "希望这个更简洁的说明能够帮助你更好地理解代码的功能、效果以及解决的问题! 如果你想了解更详细的解释，可以回顾一下之前更长的回复，里面有更深入的分析和例子。"
      ],
      "metadata": {
        "id": "9xAFIRkWDUUX"
      },
      "id": "9xAFIRkWDUUX"
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入操作系统库，用于处理文件路径等操作系统相关的功能\n",
        "import os\n",
        "\n",
        "\n",
        "# 导入 langchain 库中的 create_history_aware_retriever 和 create_retrieval_chain 函数，用于创建支持历史记录的检索器和检索链\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "# 导入 langchain 库中的 create_stuff_documents_chain 函数，用于创建将所有检索到的文档塞入单个上下文的文档组合链\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# 导入 langchain_community.vectorstores 库中的 Chroma 类，用于使用 Chroma 向量数据库\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# 导入 langchain_core.messages 库中的 HumanMessage 和 SystemMessage 类，用于构建对话消息\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# 导入 langchain_core.prompts 库中的 ChatPromptTemplate 和 MessagesPlaceholder 类，用于创建聊天提示模板和消息占位符\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "# 导入 langchain_openai 库中的 ChatOpenAI 和 OpenAIEmbeddings 类，用于使用 OpenAI 的聊天模型和嵌入模型\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "\n",
        "\n",
        "# 定义持久化目录\n",
        "# 获取当前文件所在目录\n",
        "current_dir = os.getcwd()\n",
        "# 构建Chroma数据库的持久化目录路径，位于当前目录下的 db/chroma_db_with_metadata 文件夹\n",
        "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
        "\n",
        "# 定义嵌入模型\n",
        "# 使用 OpenAIEmbeddings 创建嵌入模型实例，模型为 \"text-embedding-3-small\"\n",
        "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "# 词嵌入（代理方式）\n",
        "embeddings_model = OpenAIEmbeddings(\n",
        "    api_key=\"sk-paSHgQoVeKag1rou9d81Fa2f534940C1Ba394f02C45aF3D2\",\n",
        "    base_url=\"https://vip.apiyi.com/v1\",\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "\n",
        "# 加载已存在的向量存储\n",
        "# 使用 Chroma类加载持久化目录中的向量数据库，并使用之前定义的嵌入模型\n",
        "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
        "\n",
        "# 创建检索器\n",
        "# 使用 as_retriever 方法从 Chroma 数据库创建检索器\n",
        "# search_type=\"similarity\" 指定使用相似度搜索\n",
        "# search_kwargs={\"k\": 3} 指定搜索时返回最相似的 3 个文档\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        ")\n",
        "\n",
        "# 创建 ChatOpenAI 模型\n",
        "# 使用 ChatOpenAI 类创建聊天模型实例，模型为 \"gpt-4o\"\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# 上下文化问题提示\n",
        "# 定义系统提示，用于指示 AI 根据聊天历史和最新的用户问题，生成一个无需聊天历史也能理解的独立问题\n",
        "# 解决问题：当用户的问题依赖于上下文时，确保 AI 可以正确理解用户最新的问题\n",
        "# 达到的效果：使 AI 能够处理带有上下文依赖的问题，提升对话的连贯性\n",
        "contextualize_q_system_prompt = (\n",
        "    \"给定聊天历史和最新的用户问题，\"\n",
        "    \"最新的用户问题可能引用聊天历史中的上下文，\"\n",
        "    \"请构建一个可以独立理解的问题，无需聊天历史。\"\n",
        "    \"不要回答问题，只需在需要时改述问题，否则按原样返回。\"\n",
        ")\n",
        "\n",
        "# 创建用于上下文化的提示模板\n",
        "# 使用 ChatPromptTemplate.from_messages 创建聊天提示模板\n",
        "# 包括系统消息（contextualize_q_system_prompt）、消息占位符（chat_history）和人类消息（input）\n",
        "# 解决问题：结构化提示，方便传入聊天历史和用户输入，并指示 AI 执行上下文理解和问题改述\n",
        "# 达到的效果：定义了 AI 如何接收和处理上下文信息以生成独立问题\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 创建历史感知检索器\n",
        "# 使用 create_history_aware_retriever 函数创建历史感知检索器\n",
        "# 参数包括聊天模型 (llm)、检索器 (retriever) 和上下文提示模板 (contextualize_q_prompt)\n",
        "# 解决问题：结合聊天模型和检索器，使检索过程能够理解和利用对话历史\n",
        "# 达到的效果：检索器不仅能基于当前问题检索，还能考虑之前的对话内容，提高检索的准确性\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# 回答问题提示\n",
        "# 定义系统提示，用于指示 AI 使用检索到的上下文来回答问题，并限制答案的长度和处理未知答案的情况\n",
        "# 解决问题：指导 AI 如何基于检索到的信息生成答案，并处理知识库中没有答案的情况\n",
        "# 达到的效果：确保 AI 能够根据提供的上下文给出简洁明了的答案，并在无法回答时给出明确的提示\n",
        "qa_system_prompt = (\n",
        "    \"你是一个问答任务的助手。 使用\"\n",
        "    \"以下检索到的上下文片段来回答问题。 如果你不知道答案，就说你\"\n",
        "    \"不知道。 答案最多使用三句话，并保持答案简洁。\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# 创建用于回答问题的提示模板\n",
        "# 使用 ChatPromptTemplate.from_messages 创建聊天提示模板\n",
        "# 包括系统消息（qa_system_prompt）、消息占位符（chat_history）和人类消息（input）\n",
        "# 解决问题：结构化提示，方便传入上下文信息、聊天历史和用户输入，并指示 AI 如何生成答案\n",
        "# 达到的效果：定义了 AI 如何接收上下文并生成最终答案\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 创建用于组合文档以进行问答的链\n",
        "# 使用 create_stuff_documents_chain 函数创建文档组合链\n",
        "# 参数包括聊天模型 (llm) 和问答提示模板 (qa_prompt)\n",
        "# 解决问题：将检索到的多个文档整合为一个上下文，方便 AI 理解和提取信息\n",
        "# 达到的效果：AI 可以一次性处理所有相关的检索结果，并从中提取答案\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "# 创建检索链\n",
        "# 使用 create_retrieval_chain 函数创建检索链，将历史感知检索器和问答链组合起来\n",
        "# 参数包括历史感知检索器 (history_aware_retriever) 和问答链 (question_answer_chain)\n",
        "# 解决问题：将检索和问答两个核心步骤连接起来，形成一个完整的 RAG (Retrieval-Augmented Generation) 流程\n",
        "# 达到的效果：构建了一个完整的问答系统，能够处理带有上下文的查询，检索相关文档，并生成答案\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "# 模拟持续对话的函数\n",
        "def continual_chat():\n",
        "    print(\"开始与 AI 聊天! 输入 'exit' 结束对话。\")\n",
        "    chat_history = []  # 用于收集聊天历史记录 (消息序列)\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "        # 通过检索链处理用户的查询\n",
        "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
        "        # 显示 AI 的回复\n",
        "        print(f\"AI: {result['answer']}\")\n",
        "        # 更新聊天历史\n",
        "        chat_history.append(HumanMessage(content=query))\n",
        "        chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
        "\n",
        "\n",
        "# 主函数，用于启动持续对话\n",
        "if __name__ == \"__main__\":\n",
        "    continual_chat()"
      ],
      "metadata": {
        "id": "MqbfoDj-BtxJ",
        "outputId": "cfbaecf8-c2c1-49ba-99e9-7710fbe8b602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "id": "MqbfoDj-BtxJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0655fc263eb0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 定义嵌入模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 使用 OpenAIEmbeddings 创建嵌入模型实例，模型为 \"text-embedding-3-small\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-3-small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 加载已存在的向量存储\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_proxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_proxy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_async_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1b3d08692c66b50e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# 导入 os 模块，用于处理文件路径和目录\n",
        "import os\n",
        "\n",
        "# 从 langchain 库的 hub 模块导入 hub，用于访问 prompt 的 hub\n",
        "from langchain import hub\n",
        "# 从 langchain 库的 agents 模块导入 AgentExecutor 和 create_react_agent，用于创建和运行 ReAct 代理\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "# 从 langchain 库的 chains 模块导入 create_history_aware_retriever 和 create_retrieval_chain，用于创建检索链\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "# 从 langchain 库的 chains.combine_documents 模块导入 create_stuff_documents_chain，用于创建 stuff 文档链\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# 从 langchain_community 库的 vectorstores 模块导入 Chroma，用于使用 Chroma 向量数据库\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# 从 langchain_core 库的 messages 模块导入 AIMessage 和 HumanMessage，用于表示 AI 消息和人类消息\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "# 从 langchain_core 库的 prompts 模块导入 ChatPromptTemplate 和 MessagesPlaceholder，用于创建聊天提示模板和消息占位符\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "# 从 langchain_core 库的 tools 模块导入 Tool，用于创建工具\n",
        "from langchain_core.tools import Tool\n",
        "# 从 langchain_openai 库导入 ChatOpenAI 和 OpenAIEmbeddings，用于使用 OpenAI 的聊天模型和嵌入模型\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ],
      "id": "1b3d08692c66b50e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "35628c16-da84-41f6-f692-17b6e9a0cdda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "The directory /content/rag/db/chroma_db_with_metadata does not exist. Please check the path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7e334a96c232>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# 如果目录不存在，抛出异常，提示用户检查路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;34mf\"The directory {persistent_directory} does not exist. Please check the path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: The directory /content/rag/db/chroma_db_with_metadata does not exist. Please check the path."
          ]
        }
      ],
      "source": [
        "#\n",
        "\n",
        "\"\"\"\n",
        "1. 配置 Chroma 向量数据库目录\n",
        "\n",
        "定义 Chroma 向量数据库的持久化存储目录。\n",
        "向量数据库用于存储和检索文档嵌入，以便进行语义搜索和 RAG (Retrieval-Augmented Generation)。\n",
        "\"\"\"\n",
        "# Load the existing Chroma vector store\n",
        "# 获取当前脚本的目录\n",
        "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "current_dir = os.getcwd()\n",
        "# 构建数据库目录的路径，假设数据库位于rag目录下的db目录中\n",
        "db_dir = os.path.join(current_dir, \"rag\", \"db\")\n",
        "# 构建 Chroma 数据库持久化存储的完整路径\n",
        "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
        "\n",
        "\"\"\"\n",
        "检查并加载现有的Chroma向量数据库\n",
        "\n",
        "检查指定的持久化目录是否存在 Chroma 向量数据库。\n",
        "如果存在，则加载现有的数据库；如果不存在，则抛出 FileNotFoundError 异常，提示用户检查路径。\n",
        "这避免了每次运行代码时都重新创建向量数据库，提高了效率。\n",
        "\"\"\"\n",
        "# Check if the Chroma vector store already exists\n",
        "if os.path.exists(persistent_directory):\n",
        "    print(\"Loading existing vector store...\")\n",
        "    # 如果目录存在，则加载已有的Chroma向量数据库，embedding_function 初始设置为 None，稍后会更新\n",
        "    db = Chroma(persist_directory=persistent_directory,\n",
        "                embedding_function=None)\n",
        "else:\n",
        "    # 如果目录不存在，抛出异常，提示用户检查路径\n",
        "    raise FileNotFoundError(\n",
        "        f\"The directory {persistent_directory} does not exist. Please check the path.\"\n",
        "    )\n",
        "\n",
        "\"\"\"\n",
        "2. 定义嵌入模型\n",
        "\n",
        "使用 OpenAIEmbeddings 定义文档嵌入模型。\n",
        "嵌入模型用于将文本转换为向量，以便在向量空间中进行相似性搜索。\n",
        "这里使用 'text-embedding-3-small' 模型，这是一个性价比高的嵌入模型。\n",
        "\"\"\"\n",
        "# Define the embedding model\n",
        "# 创建 OpenAIEmbeddings 实例，使用 'text-embedding-3-small' 模型\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "\"\"\"\n",
        "3. 重新加载 Chroma 向量数据库并配置嵌入函数\n",
        "\n",
        "使用之前定义的嵌入模型重新加载 Chroma 向量数据库。\n",
        "在第一次加载时，embedding_function 设置为 None，现在使用 OpenAIEmbeddings 对象进行更新。\n",
        "确保向量数据库使用正确的嵌入函数进行操作。\n",
        "\"\"\"\n",
        "# Load the existing vector store with the embedding function\n",
        "# 重新加载 Chroma 向量数据库，这次使用 OpenAIEmbeddings 作为嵌入函数\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "\"\"\"\n",
        "4. 创建检索器 (Retriever)\n",
        "\n",
        "从 Chroma 向量数据库创建检索器。\n",
        "检索器用于根据用户查询在向量数据库中搜索相关文档。\n",
        "`search_type=\"similarity\"` 指定使用相似性搜索，`search_kwargs={\"k\": 3}` 指定返回最相似的 3 个文档。\n",
        "\"\"\"\n",
        "# Create a retriever for querying the vector store\n",
        "# 从 Chroma 数据库创建检索器，用于执行相似性搜索\n",
        "# `search_type` 指定搜索类型为 \"similarity\" (相似性搜索)\n",
        "# `search_kwargs` 设置搜索参数，`k=3` 表示返回最相关的 3 个文档\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "5. 创建 ChatOpenAI 模型\n",
        "\n",
        "初始化 ChatOpenAI 模型，使用 'gpt-4o' 模型。\n",
        "这是用于生成答案和处理自然语言对话的大型语言模型。\n",
        "'gpt-4o' 是一个先进的多模态模型，具有强大的文本处理能力。\n",
        "\"\"\"\n",
        "# Create a ChatOpenAI model\n",
        "# 初始化 ChatOpenAI 模型，使用 'gpt-4o' 模型\n",
        "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# OpenAI API调用（代理方式）\n",
        "llm = ChatOpenAI(\n",
        "    api_key=\"XXX\",\n",
        "    base_url=\"https://vip.apiyi.com/v1\",\n",
        "    model=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "6. 上下文情境化问题提示 (Contextualize Question Prompt)\n",
        "\n",
        "定义一个系统提示，用于指导 AI 如何根据聊天历史记录和最新的用户问题，生成一个独立的、无需上下文也能理解的问题。\n",
        "这个提示的目的是让 AI 能够处理在对话中可能出现的指代和省略，确保检索器总是能获得清晰明确的查询。\n",
        "\"\"\"\n",
        "# Contextualize question prompt\n",
        "# 系统提示，用于指导 AI 根据聊天历史和用户问题，生成一个独立的、无需上下文也能理解的问题\n",
        "# 目标是让 AI 能够处理对话中的上下文依赖，生成清晰的查询\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, just \"\n",
        "    \"reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "7. 创建上下文情境化问题提示模板\n",
        "\n",
        "使用 ChatPromptTemplate.from_messages 创建提示模板，该模板包含系统提示、聊天历史记录的占位符和用户输入的占位符。\n",
        "MessagesPlaceholder(\"chat_history\") 用于在运行时动态地插入聊天历史记录。\n",
        "\"\"\"\n",
        "# Create a prompt template for contextualizing questions\n",
        "# 使用 ChatPromptTemplate 创建提示模板，包含系统提示、聊天历史记录占位符和用户输入占位符\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "8. 创建历史感知检索器 (History-Aware Retriever)\n",
        "\n",
        "使用 create_history_aware_retriever 函数，将 LLM、基础检索器和上下文情境化问题提示模板组合在一起，创建一个历史感知检索器。\n",
        "这个检索器能够在检索文档之前，先利用 LLM 和聊天历史记录来提炼用户的问题，从而提高检索的准确性。\n",
        "\"\"\"\n",
        "# Create a history-aware retriever\n",
        "# 使用 create_history_aware_retriever 创建历史感知检索器\n",
        "# 结合 LLM, 基础检索器和上下文情境化问题提示模板\n",
        "# 使得检索器能够根据聊天历史记录来优化检索query\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "9. 答案问题提示 (Answer Question Prompt)\n",
        "\n",
        "定义一个系统提示，用于指导 AI 如何使用检索到的上下文来回答用户问题。\n",
        "该提示指示 AI 作为一个问题解答助手，使用检索到的上下文，并在不知道答案时明确表示“我不知道”。\n",
        "同时，限制答案长度为最多三句话，保持简洁。\n",
        "\"\"\"\n",
        "# Answer question prompt\n",
        "# 系统提示，用于指导 AI 如何使用检索到的上下文来回答问题\n",
        "# 指示 AI 作为问题解答助手，使用上下文，并在不知道答案时回答 \"I don't know\"\n",
        "# 限制答案长度为最多三句话，保持简洁\n",
        "qa_system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. Use \"\n",
        "    \"the following pieces of retrieved context to answer the \"\n",
        "    \"question. If you don't know the answer, just say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the answer \"\n",
        "    \"concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "10. 创建答案问题提示模板\n",
        "\n",
        "使用 ChatPromptTemplate.from_messages 创建答案问题提示模板，包含系统提示、聊天历史记录占位符和用户输入占位符。\n",
        "虽然答案问题提示模板中也包含了 `MessagesPlaceholder(\"chat_history\")`，但在当前的 `create_stuff_documents_chain` 使用方式下，聊天历史记录实际上并没有直接被答案生成链使用。\n",
        "这里保留 `MessagesPlaceholder(\"chat_history\")` 可能是为了未来扩展，或者保持提示模板结构的一致性。\n",
        "\"\"\"\n",
        "# Create a prompt template for answering questions\n",
        "# 使用 ChatPromptTemplate 创建答案问题提示模板，包含系统提示、聊天历史记录占位符和用户输入占位符\n",
        "# 注意：虽然这里也包含了 chat_history 占位符，但在当前的 chain 结构中，答案生成链可能并没有直接使用 chat_history\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "11. 创建文档组合链 (Combine Documents Chain)\n",
        "\n",
        "使用 create_stuff_documents_chain 函数，将 LLM 和答案问题提示模板组合在一起，创建一个文档组合链。\n",
        "`create_stuff_documents_chain` 使用 \"stuff\" 方式，将所有检索到的文档合并成一个字符串，一次性送入 LLM 进行处理。\n",
        "这个链负责接收检索器返回的文档，并将它们与用户问题一起传递给 LLM，以生成最终答案。\n",
        "\"\"\"\n",
        "# Create a chain to combine documents for question answering\n",
        "# 使用 create_stuff_documents_chain 创建文档组合链\n",
        "# `create_stuff_documents_chain` 使用 \"stuff\" 方式，将所有检索到的文档一次性喂给 LLM\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "\"\"\"\n",
        "12. 创建 RAG 链 (Retrieval-Augmented Generation Chain)\n",
        "\n",
        "使用 create_retrieval_chain 函数，将历史感知检索器和文档组合链组合在一起，创建 RAG 链。\n",
        "RAG 链是整个问答系统的核心，它首先使用历史感知检索器检索相关文档，然后使用文档组合链和 LLM 根据检索到的文档生成答案。\n",
        "\"\"\"\n",
        "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
        "# 使用 create_retrieval_chain 创建 RAG 链，将历史感知检索器和答案生成链组合起来\n",
        "rag_chain = create_retrieval_chain(\n",
        "    history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "13. 设置带有文档存储检索器的 ReAct Agent (ReAct Agent with Document Store Retriever)\n",
        "\n",
        "配置 ReAct Agent，使其能够利用文档存储进行检索和问题回答。\n",
        "ReAct (Reason and Act) 是一种 agent 框架，它允许 agent 在思考 (Reason) 和执行动作 (Act) 之间交替进行，从而更有效地完成复杂任务。\n",
        "这里使用预定义的 ReAct 提示 (react_docstore_prompt) 和一个工具 (Answer Question Tool)。\n",
        "\"\"\"\n",
        "# Set Up ReAct Agent with Document Store Retriever\n",
        "# 加载 ReAct Docstore Prompt\n",
        "react_docstore_prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "\"\"\"\n",
        "14. 定义工具 (Tools)\n",
        "\n",
        "创建一个工具列表，ReAct Agent 可以使用这些工具来执行不同的操作。\n",
        "这里定义了一个名为 \"Answer Question\" 的工具，该工具使用之前创建的 RAG 链 (rag_chain) 来回答问题。\n",
        "工具的描述 (description) 非常重要，ReAct Agent 会根据描述来决定何时以及如何使用工具。\n",
        "\"\"\"\n",
        "# 定义 Agent 可以使用的工具列表\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Answer Question\", # 工具名称，Agent 会根据名称来调用工具\n",
        "        # 工具的函数，这里使用 rag_chain.invoke 来执行 RAG 链，回答问题\n",
        "        func=lambda input, **kwargs: rag_chain.invoke(\n",
        "            {\"input\": input, \"chat_history\": kwargs.get(\"chat_history\", [])}\n",
        "        ),\n",
        "        # 工具的描述，描述工具的用途，ReAct Agent 会根据描述来决定何时使用该工具\n",
        "        description=\"useful for when you need to answer questions about the context\",\n",
        "    )\n",
        "]\n",
        "\n",
        "\"\"\"\n",
        "15. 创建 ReAct Agent\n",
        "\n",
        "使用 create_react_agent 函数创建 ReAct Agent。\n",
        "需要传入 LLM, 工具列表和 ReAct 提示模板。\n",
        "ReAct Agent 会根据提示模板和可用的工具，以及用户的输入，自主决定执行哪些操作来完成任务。\n",
        "\"\"\"\n",
        "# Create the ReAct Agent with document store retriever\n",
        "# 使用 create_react_agent 创建 ReAct Agent\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=react_docstore_prompt,\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "16. 创建 Agent 执行器 (Agent Executor)\n",
        "\n",
        "使用 AgentExecutor.from_agent_and_tools 创建 Agent 执行器。\n",
        "Agent 执行器是运行 Agent 的核心组件，它负责接收用户输入，调用 Agent 进行决策，执行工具，并返回最终结果。\n",
        "`handle_parsing_errors=True` 用于处理 Agent 输出解析错误的情况，`verbose=True` 开启详细日志输出，方便调试。\n",
        "\"\"\"\n",
        "# 创建 AgentExecutor，用于运行 Agent\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, handle_parsing_errors=True, verbose=True,\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "17. 主聊天循环 (Main Chat Loop)\n",
        "\n",
        "实现一个无限循环，模拟聊天对话。\n",
        "用户可以输入查询，Agent 执行器会处理查询并返回 AI 的回复。\n",
        "聊天历史记录 (chat_history) 会在每次对话后更新，以便 Agent 可以感知上下文。\n",
        "输入 \"exit\" 可以结束循环。\n",
        "\"\"\"\n",
        "# 初始化聊天历史记录列表\n",
        "chat_history = []\n",
        "# 进入无限循环，模拟聊天对话\n",
        "while True:\n",
        "    # 获取用户输入\n",
        "    query = input(\"You: \")\n",
        "    # 如果用户输入 \"exit\"，则退出循环\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    # 调用 agent_executor 的 invoke 方法，传入用户输入和聊天历史记录，获取 AI 的回复\n",
        "    response = agent_executor.invoke(\n",
        "        {\"input\": query, \"chat_history\": chat_history})\n",
        "    # 打印 AI 的回复\n",
        "    print(f\"AI: {response['output']}\")\n",
        "\n",
        "    # 更新聊天历史记录，将用户消息和 AI 消息添加到列表中\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "    chat_history.append(AIMessage(content=response[\"output\"]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}