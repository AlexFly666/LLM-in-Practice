{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "7f1bffda-72a1-421b-c30d-ba79c8f71ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.70-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.5.1-py3-none-any.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langgraph-checkpoint-sqlite\n",
            "  Downloading langgraph_checkpoint_sqlite-2.0.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.34-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.18 (from langchain-community)\n",
            "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.5)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.12-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n",
            "Collecting aiosqlite<0.21.0,>=0.20.0 (from langgraph-checkpoint-sqlite)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<0.21.0,>=0.20.0->langgraph-checkpoint-sqlite) (4.12.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.18->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.70-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.4-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tavily_python-0.5.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint_sqlite-2.0.3-py3-none-any.whl (12 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.34-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.12-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, aiosqlite, typing-inspect, tiktoken, tavily-python, pydantic-settings, langgraph-sdk, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph-checkpoint-sqlite, langgraph, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.33\n",
            "    Uninstalling langchain-core-0.3.33:\n",
            "      Successfully uninstalled langchain-core-0.3.33\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.5\n",
            "    Uninstalling langchain-text-splitters-0.3.5:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.17\n",
            "    Uninstalling langchain-0.3.17:\n",
            "      Successfully uninstalled langchain-0.3.17\n",
            "Successfully installed aiosqlite-0.20.0 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.18 langchain-community-0.3.17 langchain-core-0.3.34 langchain-openai-0.3.4 langchain-text-splitters-0.3.6 langgraph-0.2.70 langgraph-checkpoint-2.0.12 langgraph-checkpoint-sqlite-2.0.3 langgraph-sdk-0.1.51 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 tavily-python-0.5.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# LangChain V0.3版本\n",
        "!pip install langchain-community \\\n",
        "        langgraph \\\n",
        "        langchain-openai \\\n",
        "        tavily-python \\\n",
        "        langgraph-checkpoint-sqlite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将使用Tavily（搜索引擎）作为工具。要使用它，你需要获取并设置API密钥：\n",
        "# Tavily's Search API地址 https://tavily.com/\n",
        "import os\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\" # 将 API key 设置为环境变量\n"
      ],
      "metadata": {
        "id": "GbTACLEu4LeF"
      },
      "id": "GbTACLEu4LeF",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ng9gnPYEu-RI"
      },
      "id": "Ng9gnPYEu-RI",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45cb94e4b83b9b37"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 9,
      "source": [
        "# 导入相关的 LangChain 功能模块\n",
        "# 从 langchain_openai 库导入 ChatOpenAI 和 OpenAIEmbeddings，用于使用 OpenAI 的聊天模型和嵌入模型\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults  # 导入 Tavily 搜索引擎工具\n",
        "from langchain_core.messages import HumanMessage  # 导入 HumanMessage 类型，用于表示用户输入\n",
        "from langgraph.checkpoint.memory import MemorySaver  # 导入 MemorySaver，用于内存存储对话历史\n",
        "from langgraph.prebuilt import create_react_agent  # 导入 create_react_agent 函数，用于创建 React Agent"
      ],
      "id": "45cb94e4b83b9b37"
    },
    {
      "metadata": {
        "id": "7adc49e0c73e9bf4",
        "outputId": "0bbc50cb-d661-49b8-c15b-352ae221b6b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 首次对话 ---\n",
            "{'agent': {'messages': [AIMessage(content='你好，小明！很高兴认识你。北京是个充满历史和文化的地方，你最喜欢那里哪个景点呢？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 93, 'total_tokens': 143, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5f9a334b-e46c-4afc-bebd-f1db22860bb9-0', usage_metadata={'input_tokens': 93, 'output_tokens': 50, 'total_tokens': 143, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
            "----\n",
            "\n",
            "--- 第二次对话 ---\n",
            "{'agent': {'messages': [AIMessage(content='我无法提供实时天气信息，但你可以通过天气应用或网站查询北京的最新天气情况。一般来说，北京冬季寒冷干燥，夏季炎热潮湿。你今天打算出去吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 83, 'prompt_tokens': 165, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2a6b654a-4e1a-4663-b916-25cb8f5bf1af-0', usage_metadata={'input_tokens': 165, 'output_tokens': 83, 'total_tokens': 248, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
            "----\n",
            "\n",
            "--- 第三次对话 (验证记忆) ---\n",
            "{'agent': {'messages': [AIMessage(content='你叫小明。有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 267, 'total_tokens': 288, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-96ebcfb8-5ff7-41c3-932f-aedf55ec069d-0', usage_metadata={'input_tokens': 267, 'output_tokens': 21, 'total_tokens': 288, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
            "----\n",
            "\n",
            "--- 新的对话 (无记忆) ---\n",
            "{'agent': {'messages': [AIMessage(content='抱歉，我无法知道您的名字。您可以告诉我您想让我如何称呼您吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 88, 'total_tokens': 124, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a87defdf-f6b5-48db-ab78-27fe51caaa38-0', usage_metadata={'input_tokens': 88, 'output_tokens': 36, 'total_tokens': 124, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
            "----\n",
            "\n",
            "---  代码执行完成  ---\n"
          ]
        }
      ],
      "execution_count": 11,
      "source": [
        "# 创建 Agent 的组件\n",
        "\n",
        "# 初始化内存存储器，用于保存对话历史 (可选，用于有状态 Agent)\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "# OpenAI API调用（代理方式）\n",
        "model = ChatOpenAI(\n",
        "    api_key=\"sk-paSHgQoVeKag1rou9d81Fa2f534940C1Ba394f02C45aF3D2\",\n",
        "    base_url=\"https://vip.apiyi.com/v1\"\n",
        ")\n",
        "\n",
        "\n",
        "# 初始化 Tavily 搜索引擎工具，设置最大搜索结果数为 2\n",
        "search = TavilySearchResults(max_results=2)\n",
        "# 将 Tavily 搜索引擎工具放入工具列表，Agent 可以使用这些工具\n",
        "tools = [search]\n",
        "\n",
        "# 使用 `create_react_agent` 函数创建 Agent 执行器\n",
        "# 该函数会自动将模型和工具绑定，并使用 ReAct 框架构建 Agent\n",
        "# checkpointer=memory 参数用于启用对话记忆功能\n",
        "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
        "\n",
        "\n",
        "# ---  首次对话：打招呼并告知所在地 ---\n",
        "# 配置对话线程 ID，用于记忆功能，相同 thread_id 共享对话历史\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "print(\"--- 首次对话 ---\")\n",
        "# 使用 stream 方法进行流式输出，逐步显示 Agent 的响应\n",
        "for chunk in agent_executor.stream(\n",
        "    # 构造用户输入消息 HumanMessage -  用户用中文打招呼，并告知住在北京\n",
        "    {\"messages\": [HumanMessage(content=\"你好！我是小明，我住在北京。\")]}, config\n",
        "):\n",
        "    # 打印每个 chunk 的输出，展示 Agent 的思考过程\n",
        "    print(chunk)\n",
        "    print(\"----\")\n",
        "\n",
        "\n",
        "# ---  第二次对话：询问天气 ---\n",
        "print(\"\\n--- 第二次对话 ---\")\n",
        "for chunk in agent_executor.stream(\n",
        "    # 再次使用相同的 thread_id，Agent 将记住之前的对话 -  用户用中文询问天气\n",
        "    {\"messages\": [HumanMessage(content=\"我住的地方天气怎么样？\")]}, config\n",
        "):\n",
        "    print(chunk)\n",
        "    print(\"----\")\n",
        "\n",
        "\n",
        "# ---  第三次对话： 询问名字 ---\n",
        "print(\"\\n--- 第三次对话 (验证记忆) ---\")\n",
        "for chunk in agent_executor.stream(\n",
        "    # 继续使用相同的 thread_id -  用户用中文询问名字\n",
        "    {\"messages\": [HumanMessage(content=\"请问我叫什么名字？\")]}, config\n",
        "):\n",
        "    print(chunk)\n",
        "    print(\"----\")\n",
        "\n",
        "\n",
        "# ---  新的对话：更换线程 ID，验证无记忆 ---\n",
        "# 使用新的 thread_id，开启新的对话\n",
        "config_new_thread = {\"configurable\": {\"thread_id\": \"xyz123\"}}\n",
        "print(\"\\n--- 新的对话 (无记忆) ---\")\n",
        "for chunk in agent_executor.stream(\n",
        "    # 用户在新对话线程中，用中文询问名字\n",
        "    {\"messages\": [HumanMessage(content=\"我叫什么名字？\")]}, config_new_thread\n",
        "):\n",
        "    print(chunk)\n",
        "    print(\"----\")\n",
        "\n",
        "\n",
        "print(\"\\n---  代码执行完成  ---\")"
      ],
      "id": "7adc49e0c73e9bf4"
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入 getpass 和 os 模块\n",
        "# Imports getpass and os modules\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# 检查是否已设置 Tavily API 密钥，如果未设置，则提示用户输入\n",
        "# Checks if Tavily API key is set, prompts user for input if not\n",
        "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\")\n",
        "\n",
        "# （可选）设置 LangSmith 以获得最佳的可观测性\n",
        "# (Optional) Set up LangSmith for best-in-class observability\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# 从 langchain_community.tools 导入 TavilySearchResults\n",
        "# Imports TavilySearchResults from langchain_community.tools\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "\n",
        "# 实例化 TavilySearchResults 工具\n",
        "# Instantiates TavilySearchResults tool\n",
        "tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        "    # include_domains=[...],\n",
        "    # exclude_domains=[...],\n",
        "    # name=\"...\",                   # overwrite default tool name\n",
        "    # description=\"...\",            # overwrite default tool description\n",
        "    # args_schema=...,              # overwrite default args_schema: BaseModel\n",
        ")\n",
        "\n",
        "# 直接调用工具，传入查询参数\n",
        "# Directly invokes the tool with query argument\n",
        "tool.invoke({\"query\": \"What happened at the last wimbledon\"})\n",
        "\n",
        "# 创建一个模型生成的 ToolCall 用于演示目的\n",
        "# Creates a model-generated ToolCall for demo purposes\n",
        "model_generated_tool_call = {\n",
        "    \"args\": {\"query\": \"euro 2024 host nation\"},\n",
        "    \"id\": \"1\",\n",
        "    \"name\": \"tavily\",\n",
        "    \"type\": \"tool_call\",\n",
        "}\n",
        "# 调用工具并获取 ToolMessage\n",
        "# Invokes the tool and gets ToolMessage\n",
        "tool_msg = tool.invoke(model_generated_tool_call)\n",
        "\n",
        "# 打印 ToolMessage 内容的前 400 个字符 (JSON 字符串形式)\n",
        "# Prints the first 400 characters of ToolMessage content (JSON string format)\n",
        "print(tool_msg.content[:400])\n",
        "\n",
        "# 检查 ToolMessage artifact 中各项的类型\n",
        "# Checks the type of each item in ToolMessage artifact\n",
        "{k: type(v) for k, v in tool_msg.artifact.items()}\n",
        "\n",
        "# 导入 json 模块\n",
        "# Imports json module\n",
        "import json\n",
        "\n",
        "# 缩写 artifact 中的结果，并打印 JSON 格式的 artifact 信息 (缩写后的字符串)\n",
        "# Abbreviates results in artifact and prints artifact info in JSON format (abbreviated strings)\n",
        "print(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n",
        "\n",
        "# 安装 langchain[openai] 包\n",
        "# Installs langchain[openai] package\n",
        "# %pip install -qU \"langchain[openai]\" # 注释掉，避免重复安装，如果需要使用chaining功能再取消注释\n",
        "\n",
        "# 再次检查并设置 OpenAI API 密钥，如果未设置，则提示用户输入\n",
        "# Checks and sets OpenAI API key again, prompts user for input if not\n",
        "# import getpass # 注释掉，避免重复导入\n",
        "# import os # 注释掉，避免重复导入\n",
        "\n",
        "# if not os.environ.get(\"OPENAI_API_KEY\"): # 注释掉，避免重复设置，如果需要使用chaining功能再取消注释\n",
        "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \") # 注释掉，避免重复设置，如果需要使用chaining功能再取消注释\n",
        "\n",
        "# 从 langchain.chat_models 导入 init_chat_model\n",
        "# Imports init_chat_model from langchain.chat_models\n",
        "from langchain.chat_models import ChatOpenAI # 修改为ChatOpenAI，init_chat_model 在新版本中可能已弃用\n",
        "\n",
        "# 初始化聊天模型 (使用 gpt-4o-mini 模型， provider 为 openai)\n",
        "# Initializes chat model (using gpt-4o-mini model, provider is openai)\n",
        "llm = ChatOpenAI(\n",
        "    api_key=\"sk-paSHgQoVeKag1rou9d81Fa2f534940C1Ba394f02C45aF3D2\",\n",
        "    base_url=\"https://vip.apiyi.com/v1\",\n",
        "    model_name=\"gpt-4o-mini\"\n",
        ")\n",
        "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=os.environ.get(\"OPENAI_API_KEY\")) # 修改为 ChatOpenAI 实例化方式，并显式传入API key\n",
        "\n",
        "# 导入 datetime 模块\n",
        "# Imports datetime module\n",
        "import datetime\n",
        "\n",
        "# 从 langchain_core.prompts 导入 ChatPromptTemplate\n",
        "# Imports ChatPromptTemplate from langchain_core.prompts\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# 从 langchain_core.runnables 导入 RunnableConfig 和 chain\n",
        "# Imports RunnableConfig and chain from langchain_core.runnables\n",
        "from langchain_core.runnables import RunnableConfig, chain\n",
        "\n",
        "# 获取今天的日期并格式化\n",
        "# Gets today's date and formats it\n",
        "today = datetime.datetime.today().strftime(\"%D\")\n",
        "# 定义提示模板\n",
        "# Defines prompt template\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\", f\"You are a helpful assistant. The date today is {today}.\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 将工具绑定到语言模型，指定 tool_choice 以强制模型调用该工具\n",
        "# Binds tool to language model, specifies tool_choice to force model to call this tool\n",
        "llm_with_tools = llm.bind_tools([tool])\n",
        "\n",
        "# 创建 LLM 链\n",
        "# Creates LLM chain\n",
        "llm_chain = prompt | llm_with_tools\n",
        "\n",
        "# 定义工具链函数\n",
        "# Defines tool chain function\n",
        "@chain\n",
        "def tool_chain(user_input: str, config: RunnableConfig):\n",
        "    input_ = {\"user_input\": user_input}\n",
        "    ai_msg = llm_chain.invoke(input_, config=config)\n",
        "    tool_msgs = tool.batch(ai_msg.tool_calls, config=config)\n",
        "    return llm_chain.invoke({**input_, \"messages\": [ai_msg, *tool_msgs]}, config=config)\n",
        "\n",
        "# 调用工具链，查询 “who won the last womens singles wimbledon”\n",
        "# Invokes tool chain, querying \"who won the last womens singles wimbledon\"\n",
        "tool_chain.invoke(\"who won the last womens singles wimbledon\")\n"
      ],
      "metadata": {
        "id": "LjZ5GLrtwxeq",
        "outputId": "f32371af-db6f-4cfd-af04-d5b1b1a4758b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "id": "LjZ5GLrtwxeq",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"url\": \"https://www.sportingnews.com/uk/football/news/list-euros-host-nations-uefa-european-championship-countries/85f8069d69c9f4ecd00c4900\", \"content\": \"Complete list of Euros host nations: Countries to have held UEFA European Championship and how many times Germany has staged ahead of 2024 The 2024 UEFA European Championship, more commonly known as Euro 2024, will mark the 17th edition of the \n",
            "{\n",
            "  \"query\": \"euro 2024 host nation\",\n",
            "  \"follow_up_questions\": \"None\",\n",
            "  \"answer\": \"Germany is the host nation for Euro 2024, with matches taking place across ten cities: Berlin, Cologne, Munich, Frankfurt, Hamburg, Dortmund, Leipzig, Gelsenkirchen, Stuttgart, and D\\u00fcsseldorf. The tou\",\n",
            "  \"images\": \"['https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://www.dfb.de/fileadmin/_processed_/202110/csm_248219-EURO2024_HC_Gelsenkirchen_FC_CMYK_266df023cc.png', 'https://th\",\n",
            "  \"results\": \"[{'title': 'Complete list of Euros host nations: Countries to have held UEFA ...', 'url': 'https://www.sportingnews.com/uk/football/news/list-euros-host-nations-uefa-european-championship-countries/85\",\n",
            "  \"response_time\": \"0.97\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-62a3eb9a7049>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# 将工具绑定到语言模型，指定 tool_choice 以强制模型调用该工具\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Binds tool to language model, specifies tool_choice to force model to call this tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mllm_with_tools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# 创建 LLM 链\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mbind_tools\u001b[0;34m(self, tools, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     ) -> Runnable[LanguageModelInput, BaseMessage]:\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m     def with_structured_output(\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}