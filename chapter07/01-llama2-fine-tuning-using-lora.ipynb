{"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"096ba04a1d7f45ec84f06a7a9f8276b7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"251ff2c9141c4da99f51008dfbee9b04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_096ba04a1d7f45ec84f06a7a9f8276b7","placeholder":"​","style":"IPY_MODEL_d4a6149f85a14809b2ec8162722a654c","value":" 2/2 [00:12&lt;00:00,  5.40s/it]"}},"528923440b3a482c9dc8c04157c687fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7086cff6a30948e5b4af029e653f8e71","placeholder":"​","style":"IPY_MODEL_644476f799be4a3f9354e3e5c47b6feb","value":"Loading checkpoint shards: 100%"}},"5cfc4ffce7934bc983e9d1bd43b388b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_528923440b3a482c9dc8c04157c687fa","IPY_MODEL_cb6170e69e4349c792f640fab25f8cf6","IPY_MODEL_251ff2c9141c4da99f51008dfbee9b04"],"layout":"IPY_MODEL_ece5939e218d4acca4aacf3e0ca6f947"}},"644476f799be4a3f9354e3e5c47b6feb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7086cff6a30948e5b4af029e653f8e71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b21e3f35d647483b8c11937e9f715702":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb6170e69e4349c792f640fab25f8cf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d008a6ef62db4bfb9e241c35ace92811","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b21e3f35d647483b8c11937e9f715702","value":2}},"d008a6ef62db4bfb9e241c35ace92811":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4a6149f85a14809b2ec8162722a654c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ece5939e218d4acca4aacf3e0ca6f947":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://media.licdn.com/dms/image/D4D12AQGkHY89zhGDZA/article-cover_image-shrink_720_1280/0/1691431976579?e=2147483647&v=beta&t=lgk0jF-6kju4RyG68tF_4cxZklJqF3rtXrbQEcEYX7c\">\n\n\n# <b><span style='color:#F1A424'>|</span> Mission: <span style='color:#F1A424'>Llama2 Fine-Tuning</span><span style='color:#ABABAB'>\n\n***\n\n**Please consider upvoting the notebook if you found it useful** \n\nWelcome, Kaggle enthusiasts and AI pioneers! In the last year, we have witnessed an unprecedented boom in technology, particularly in the realm of Large Language Models (LLMs) and broader Artificial Intelligence (AI) systems. Tools like ChatGPT have not just entered the mainstream but have revolutionized it, reshaping how we gather and process information across various sectors.\n\nIn this notebook, we dive into the heart of this technological revolution by focusing on a specific aspect of AI - Large Language Models, with a special emphasis on a model we're referring to as \"Llama2\". You will embark on a journey to understand and master the art of fine-tuning Llama2 for text generation. By leveraging PyTorch, we aim to equip you with the skills and knowledge to harness the full potential of LLMs.\n    \nHope you enjoy it and find it useful.\n    \n\n### <b><span style='color:#F1A424'>Table of Contents</span></b> <a class='anchor' id='top'></a>\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<li> <a href=\"#install_libraries\">Install libraries</a></li>\n<li><a href=\"#import_libraries\">Import Libraries</a></li>\n<li><a href=\"#load_data\">Load Data</a></li>\n<li><a href=\"#configuration\">Configuration</a></li>\n<li><a href=\"#configure_parameters\">Configure Quantization and LORA-specific parameters</a></li>\n<li><a href=\"#load_model\">Load Model</a></li>\n<li><a href=\"#training\">Training</a></li>\n<li><a href=\"#testing\">Testing</a></li>\n<li><a href=\"#save_model\">Saving model for inference</a></li>\n</div>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Install Libraries</b><a class='anchor' id='install_libraries'></a> [↑](#top) \n\n***\n\nInstall all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top) \n\n***\n\nImport all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"GCWf-ANoqlgr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Load custom dataset</b><a class='anchor' id='load_data'></a> [↑](#top) \n\n***\n\nCustom dataset is used in this notebook. You can use any data but dataset should contain two columns with name 'prompt' and 'response'. The prompt column should contain the input text.","metadata":{}},{"cell_type":"code","source":"# Load and display the first few rows of the dataset\ndf = pd.read_csv(\"/content/drive/MyDrive/task2_10k.csv\")\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"sYsGSVRMrBWR","outputId":"c70dde02-8fd7-47c5-9b5f-ef61e8ce615c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the dataset by removing hyperlinks and mentions\nfor i in range(len(df)):\n    l = df['response'][i]\n    text = l.replace(\"<hyperlink>\",\"\")\n    l = text.replace(\"<mention>\",\"\")\n    df['response'][i] = l","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwzKfUCePBCU","outputId":"07246e64-2afe-421e-afef-43852ee26ac3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into final test data and remaining data\nfinal_test_data = df[8000:10000]\ndf = df.drop(final_test_data.index)","metadata":{"id":"veYfw2QTPlkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here test set is the validation set\n\n# Split the data into train and test sets, with 90% in the train set\ntrain_df = df.sample(frac=0.9, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)","metadata":{"id":"ogmuoMFWq_4q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Configuration</b><a class='anchor' id='configuration'></a> [↑](#top) \n\n***\n\nCentral repository for this notebook's hyperparameters.","metadata":{}},{"cell_type":"code","source":"# Set up model configuration and training parameters\nmodel_name = \"NousResearch/llama-2-7b-chat-hf\"\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-2-7b-custom\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 2\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"id":"bqfbhUZI-4c_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset('json', data_files='/content/train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='/content/test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text': [prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Configuration of Quantization and LORA parameters</b><a class='anchor' id='configure_parameters'></a> [↑](#top) \n\n***\n\nAs model size is big it is loaded in 4 bit.","metadata":{}},{"cell_type":"code","source":"# Configure quantization parameters\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Load pre-trained model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Configure LoRA-specific parameters\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"qf1qxbiF-x6p","outputId":"1bc5b55f-9866-480d-c8d6-582996a68910"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Training</b><a class='anchor' id='training'></a> [↑](#top) \n\n***\n","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=50  # Evaluate every 50 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped, \n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train the model\ntrainer.train()\n# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Testing</b><a class='anchor' id='testing'></a> [↑](#top) \n\n***\n\nTesting on test data","metadata":{}},{"cell_type":"code","source":"# Suppress logging messages to avoid unnecessary output\nlogging.set_verbosity(logging.CRITICAL)\n\n# Create text generation pipelines using the specified model and tokenizer\n# Define two pipelines with different maximum lengths\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=250)\npipe2 = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n\n# Initialize an empty list to store generated text\ngenerated_text = []\n\n# Iterate over the test data\nfor i in tqdm(range(len(final_test_data))):\n    # Extract the prompt from the test data\n    prompt = final_test_data['prompt'].iloc[i]\n    \n    # Attempt to generate text using the first pipeline with a max length of 250\n    try:\n        result = pipe(prompt)\n        # Append the generated text to the list, extracting the relevant part after '[/INST]'\n        generated_text.append(result[0]['generated_text'].split('[/INST]')[1])\n    except:\n        # If an exception occurs, try the second pipeline with a max length of 500\n        try:\n            result = pipe2(prompt)\n            # Append the generated text to the list, extracting the relevant part after '[/INST]'\n            generated_text.append(result[0]['generated_text'].split('[/INST]')[1])\n        except:\n            # If both pipelines fail, append a default placeholder text\n            generated_text.append(\"ABCD1234@#\")\n\n# The 'generated_text' list now contains the generated text for each prompt in the test data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KJI9TVUDYZr","outputId":"ce4564d6-7868-4307-90b5-64828c99407a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign the generated text to a new column 'generated_text' in the 'final_test_data' DataFrame\nfinal_test_data['generated_text'] = generated_text\n\n# Reset the index of the DataFrame for a cleaner representation in the CSV file\nfinal_test_data = final_test_data.reset_index(drop=True)\n\n# Save the DataFrame to a CSV file at the specified path\nfinal_test_data.to_csv('/content/drive/MyDrive/llama2_finetune_output_1128.csv', index=False)","metadata":{"id":"ZhqQFeAHhb_Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Saving Model for inference</b><a class='anchor' id='save_model'></a> [↑](#top) \n\n***\n","metadata":{}},{"cell_type":"code","source":"# Set the path where the merged model will be saved\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\" \n\n# Reload the base model in FP16 and configure settings\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,  \n    return_dict=True,        \n    torch_dtype=torch.float16,  \n    device_map=device_map,    \n)\n\n# Instantiate a PeftModel using the base model and the new model\nmodel = PeftModel.from_pretrained(base_model, new_model)  # Combine the base model and the fine-tuned weights\n\n# Merge the base model with LoRA weights and unload unnecessary parts\nmodel = model.merge_and_unload()  # Finalize the model by merging and unloading any redundant components\n\n# Reload the tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \ntokenizer.pad_token = tokenizer","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["5cfc4ffce7934bc983e9d1bd43b388b7","528923440b3a482c9dc8c04157c687fa","cb6170e69e4349c792f640fab25f8cf6","251ff2c9141c4da99f51008dfbee9b04","ece5939e218d4acca4aacf3e0ca6f947","7086cff6a30948e5b4af029e653f8e71","644476f799be4a3f9354e3e5c47b6feb","d008a6ef62db4bfb9e241c35ace92811","b21e3f35d647483b8c11937e9f715702","096ba04a1d7f45ec84f06a7a9f8276b7","d4a6149f85a14809b2ec8162722a654c"]},"id":"JutZuGdRq1D_","outputId":"599232c5-d743-4513-b2cc-fe5dc399b2be"},"execution_count":null,"outputs":[]}]}